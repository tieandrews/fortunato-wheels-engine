{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Price Modelling - Error Metric Logging\n",
    "\n",
    "This notebook will focus on Using the mlflow.log_artifacts functionality to save csv files for each model run under a new artifact_path in mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tempfile \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MultiLabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK, Trials\n",
    "\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "SRC_PATH = cur_dir[: cur_dir.index(\"fortunato-wheels-engine\") + len(\"fortunato-wheels-engine\")]\n",
    "if SRC_PATH not in sys.path:\n",
    "    sys.path.append(SRC_PATH)\n",
    "\n",
    "from src.data.car_ads import CarAds\n",
    "from src.logs import get_logger\n",
    "from src.data.training_preprocessing import preprocess_ads_for_training\n",
    "from src.evaluate import price_model\n",
    "from src.training.custom_components import MultiHotEncoder\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "AZURE_MLFLOW_URI = os.environ.get(\"AZURE_MLFLOW_URI\")\n",
    "mlflow.set_tracking_uri(AZURE_MLFLOW_URI)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set(rc={\"figure.figsize\": (8, 12)})\n",
    "# set context to notebook\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "plt.rcParams[\"font.family\"] = \"sans serif\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in current car adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads = CarAds()\n",
    "ads.get_car_ads(\n",
    "    data_dump=os.path.join(SRC_PATH, \"data\", \"processed\", \"car-ads-dump_2023-07-18.csv\")\n",
    ")\n",
    "# ads.get_car_ads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inital preprocessing\n",
    "ads.preprocess_ads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model features and split into train and test sets\n",
    "\n",
    "model_features = [\n",
    "    \"age_at_posting\",\n",
    "    \"mileage_per_year\",\n",
    "    \"make\",\n",
    "    \"model\",\n",
    "    \"price\",\n",
    "    \"wheel_system\",\n",
    "    \"options_list\",\n",
    "]\n",
    "\n",
    "\n",
    "# preprocess ads for training\n",
    "preprocessed_ads = preprocess_ads_for_training(\n",
    "    ads.df, model_features=model_features, exclude_new_vehicle_ads=True\n",
    ")\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    preprocessed_ads,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=preprocessed_ads[\"model\"],\n",
    ")\n",
    "\n",
    "# with features selected drop all with null values\n",
    "train_df = train_df[model_features].dropna().reset_index(drop=True)\n",
    "test_df = test_df[model_features].dropna().reset_index(drop=True)\n",
    "\n",
    "X_train = train_df.drop(columns=[\"price\"])\n",
    "y_train = train_df[\"price\"]\n",
    "X_test = test_df.drop(columns=[\"price\"])\n",
    "y_test = test_df[\"price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"age_at_posting\", \"mileage_per_year\"]\n",
    "\n",
    "categorical_features = [\"model\", \"wheel_system\", \"make\"]\n",
    "\n",
    "multi_label_features = [\"options_list\"]\n",
    "\n",
    "# make column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "        (\"multi\", MultiHotEncoder(), multi_label_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# what version of the model to download/evaluate\n",
    "PRICE_PREDICTION_MODEL_VER = 3\n",
    "PRICE_PREDICTION_MODEL_PATH = os.path.join(\n",
    "    os.pardir,\n",
    "    \"models\",\n",
    "    \"all-vehicles-price-prediction\",\n",
    "    str(PRICE_PREDICTION_MODEL_VER),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(\n",
    "    # assumed running from root of repo\n",
    "    path=os.path.join(\n",
    "        os.pardir, \"src\", \"deployment\", \"price_prediction_api\", \"config.json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "price_model = Model(\n",
    "    ws, \"all-vehicles-price-prediction\", version=PRICE_PREDICTION_MODEL_VER\n",
    ")\n",
    "\n",
    "# try:\n",
    "#     price_model.download(\n",
    "#         target_dir=PRICE_PREDICTION_MODEL_PATH,\n",
    "#         exist_ok=True,\n",
    "# )\n",
    "# except WebserviceException as e:\n",
    "#     print(f\"model has already been downloaded: {e}\")\n",
    "\n",
    "price_model.download(\n",
    "    target_dir=PRICE_PREDICTION_MODEL_PATH,\n",
    "    exist_ok=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from file\n",
    "model_path = os.path.join(PRICE_PREDICTION_MODEL_PATH, \"model\", \"model.pkl\")\n",
    "\n",
    "with open(model_path, \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# add predicted price to test_df, round to 1 decimal place\n",
    "full_df = test_df.copy(deep=True).assign(predicted_price=y_pred.round(1))\n",
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_model.calculate_evaluation_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_model.calculate_evaluation_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_model.calculate_evaluation_metrics_by_make(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_model.calculate_evaluation_metrics_by_model(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_train_data_metrics(train_df.head(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_train_data_metrics(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Hyperopt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first metric is the one to minimize\n",
    "metrics = [\"neg_mean_absolute_percentage_error\", \"neg_root_mean_squared_error\", \"r2\"]\n",
    "\n",
    "\n",
    "def objective(params):\n",
    "    classifier_type = params[\"type\"]\n",
    "    del params[\"type\"]\n",
    "    if classifier_type == \"gradient_boosting\":\n",
    "        clf = GradientBoostingRegressor(**params)\n",
    "    elif classifier_type == \"xgboost\":\n",
    "        clf = XGBRegressor(**params)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"regressor\", clf),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # manually run cross_validate and get train/test rmse, mape, and r2\n",
    "    model_cv_results = (\n",
    "        pd.DataFrame(\n",
    "            cross_validate(\n",
    "                pipe,\n",
    "                X_train.head(1000),\n",
    "                y_train.head(1000),\n",
    "                cv=5,\n",
    "                scoring=metrics,\n",
    "                return_train_score=True,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "        )\n",
    "        .agg([\"mean\", \"std\"])\n",
    "        .T\n",
    "    )\n",
    "\n",
    "    # log metrics to mlflow\n",
    "    with mlflow.start_run():\n",
    "        # log train and test for each metric\n",
    "        for m in metrics:\n",
    "            mlflow.log_metric(\n",
    "                f\"{m}_train_mean\", model_cv_results.loc[f\"train_{m}\"][\"mean\"]\n",
    "            )\n",
    "            mlflow.log_metric(\n",
    "                f\"{m}_test_mean\", model_cv_results.loc[f\"test_{m}\"][\"mean\"]\n",
    "            )\n",
    "            mlflow.log_metric(\n",
    "                f\"{m}_train_std\", model_cv_results.loc[f\"train_{m}\"][\"std\"]\n",
    "            )\n",
    "            mlflow.log_metric(f\"{m}_test_std\", model_cv_results.loc[f\"test_{m}\"][\"std\"])\n",
    "\n",
    "        # log params\n",
    "        mlflow.log_params(params)\n",
    "        # log the type of model\n",
    "        mlflow.log_param(\"model_type\", classifier_type)\n",
    "\n",
    "        fit_model = pipe.fit(X_train.head(1000), y_train.head(1000))\n",
    "\n",
    "        # log model\n",
    "        mlflow.sklearn.log_model(\n",
    "            fit_model,\n",
    "            \"model\",\n",
    "            signature=infer_signature(X_train.head(1000), y_train.head(1000)),\n",
    "        )\n",
    "\n",
    "        # predict on test set\n",
    "        y_pred = fit_model.predict(X_test.head(1000))\n",
    "\n",
    "        # add predicted price to test_df, round to 1 decimal place\n",
    "        full_df = (\n",
    "            test_df.head(1000).copy(deep=True).assign(predicted_price=y_pred.round(1))\n",
    "        )\n",
    "\n",
    "        # calculate evaluation metrics by model\n",
    "        metrics_by_model = price_model.calculate_evaluation_metrics_by_model(full_df)\n",
    "\n",
    "        # calculate evaluation metrics by make\n",
    "        metrics_by_make = price_model.calculate_evaluation_metrics_by_make(full_df)\n",
    "\n",
    "        # calculate training data metrics\n",
    "        train_data_metrics = price_model.calculate_train_data_metrics(\n",
    "            train_df.head(1000)\n",
    "        )\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            # Save model metrics to CSV file\n",
    "            model_metrics_fname = os.path.join(tmpdir, \"metrics_by_model.csv\")\n",
    "            metrics_by_model.to_csv(model_metrics_fname, index=False)\n",
    "\n",
    "            # Save make metrics to CSV file\n",
    "            make_metrics_fname = os.path.join(tmpdir, \"metrics_by_make.csv\")\n",
    "            metrics_by_make.to_csv(make_metrics_fname, index=False)\n",
    "\n",
    "            # Save train metrics to CSV file\n",
    "            train_metrics_fname = os.path.join(tmpdir, \"train_data_metrics.csv\")\n",
    "            train_data_metrics.to_csv(train_metrics_fname, index=False)\n",
    "\n",
    "            # Log metrics files as artifacts\n",
    "            mlflow.log_artifacts(tmpdir, artifact_path=\"evaluate/\")\n",
    "\n",
    "    # make negative mape positive so it minimizes it\n",
    "    result = {\n",
    "        \"loss\": -model_cv_results.loc[\"test_\" + metrics[0]][\"mean\"],\n",
    "        \"status\": STATUS_OK,\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = hp.choice(\n",
    "    \"classifier_type\",\n",
    "    [\n",
    "        {\n",
    "            \"type\": \"gradient_boosting\",\n",
    "            \"max_features\": hp.choice(\"max_features\", [\"sqrt\", \"log2\"]),\n",
    "            \"max_depth\": hp.uniformint(\"max_depth\", 15, 30),\n",
    "            \"min_samples_split\": hp.uniformint(\"dtree_min_samples_split\", 20, 40),\n",
    "            \"n_estimators\": hp.uniformint(\"n_estimators\", 150, 300),\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.set_experiment(\"price-prediction-v3-gradboost\")\n",
    "mlflow.set_experiment(\"sandbox\")\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "\n",
    "search_algorithm = tpe.suggest\n",
    "\n",
    "best_hyperparams = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=search_algorithm,\n",
    "    max_evals=1,\n",
    "    trials=Trials(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fwhleng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
